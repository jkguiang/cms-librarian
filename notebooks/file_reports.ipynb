{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59562064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "REPORT_SUBJ = \"user\"\n",
    "REPORT_DATE = \"2022-02-17\" # YEAR-MM-DD\n",
    "\n",
    "HADOOP_PATH = f\"cms/{REPORT_SUBJ}/store\"\n",
    "\n",
    "BINS = [\n",
    "    0.1, 1, 10, 1e2, \n",
    "    1e3, 1e4, 1e5, \n",
    "    1e6, 1e7, 1e8, \n",
    "    1e9, 1e10, 1e11, \n",
    "    1e12, 1e13\n",
    "]\n",
    "LABELS = [\n",
    "    \"0B\", \"1B\", \"10B\", \"100B\", \n",
    "    \"1KB\", \"10KB\", \"100KB\", \n",
    "    \"1MB\", \"10MB\", \"100MB\",\n",
    "    \"1GB\", \"10GB\", \"100GB\",\n",
    "    \"1TB\", \"10TB\"\n",
    "]\n",
    "BYTES_LOOKUP = dict(zip(LABELS, BINS))\n",
    "\n",
    "rcParams[\"legend.fontsize\"] = 11\n",
    "rcParams[\"legend.labelspacing\"] = 0.2\n",
    "rcParams[\"hatch.linewidth\"] = 0.5\n",
    "rcParams[\"axes.xmargin\"] = 0.0  # rootlike, no extra padding within x axis\n",
    "rcParams[\"axes.labelsize\"] = \"x-large\"\n",
    "rcParams[\"axes.formatter.use_mathtext\"] = True\n",
    "rcParams[\"legend.framealpha\"] = 0.65\n",
    "rcParams[\"axes.labelsize\"] = \"x-large\"\n",
    "rcParams[\"axes.titlesize\"] = \"large\"\n",
    "rcParams[\"xtick.labelsize\"] = \"large\"\n",
    "rcParams[\"ytick.labelsize\"] = \"large\"\n",
    "rcParams[\"figure.subplot.hspace\"] = 0.1\n",
    "rcParams[\"figure.subplot.wspace\"] = 0.1\n",
    "rcParams[\"figure.subplot.right\"] = 0.96\n",
    "rcParams[\"figure.max_open_warning\"] = 0\n",
    "rcParams[\"figure.dpi\"] = 100\n",
    "rcParams[\"axes.formatter.limits\"] = [-5, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4a6d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "reports_file = f\"../hadoop_reports/{REPORT_SUBJ}_reports/{REPORT_DATE}/all_user_files.txt.gz\"\n",
    "chunk_dir = f\"../hadoop_reports/{REPORT_SUBJ}_reports/{REPORT_DATE}/parquet_chunks\"\n",
    "\n",
    "os.makedirs(chunk_dir, exist_ok=True)\n",
    "if len(glob.glob(f\"{chunk_dir}/*.parquet\")) == 0:\n",
    "    df_chunks = pd.read_csv(\n",
    "        reports_file, \n",
    "        header=None, \n",
    "        delim_whitespace=True, \n",
    "        chunksize=200000, \n",
    "        error_bad_lines=False,\n",
    "        warn_bad_lines=False\n",
    "    )\n",
    "    for i, df_chunk in tqdm(enumerate(df_chunks)):\n",
    "        # Set columns for each chunk (these are space-separated output of hdfs dfs -ls -R /cms/store/...)\n",
    "        df_chunk.columns = [\"junk1\", \"junk2\", \"junk3\", \"user\", \"bytes\", \"date\", \"junk4\", \"ext\"]\n",
    "        # Select only the interesting columns\n",
    "        df_chunk = df_chunk.reset_index(drop=True)[[\"user\",\"bytes\",\"date\",\"ext\"]]\n",
    "        # Drop files w/ bytes == 0 (these are directories)\n",
    "        df_chunk = df_chunk[df_chunk[\"bytes\"] > 0]\n",
    "        # Grab the file extensions specfically\n",
    "        df_chunk[df_chunk.ext.str.contains(\".\")][\"ext\"] = df_chunk[\"ext\"].str.rsplit(\".\", 1).str[-1]\n",
    "        df_chunk[~df_chunk.ext.str.contains(\".\")][\"ext\"] = \"None\"\n",
    "        df_chunk = df_chunk.reset_index(drop=True)\n",
    "        # Translate string dates to datetime objects (more space efficient)\n",
    "        df_chunk[\"date\"] = pd.to_datetime(df_chunk[\"date\"])\n",
    "        # Translate remaining string columns to categoricals (more space efficient)\n",
    "        df_chunk[\"ext\"] = df_chunk[\"ext\"].astype(\"category\")\n",
    "        df_chunk[\"user\"] = df_chunk[\"user\"].astype(\"category\")\n",
    "        # Save chunk to a parquet file\n",
    "        df_chunk.to_parquet(f\"{chunk_dir}/chunk_{i:04d}.parquet\")\n",
    "\n",
    "# Recursively read all parquet_files and concat them, preserving categoricals\n",
    "df = pd.read_parquet(chunk_dir)\n",
    "# Make all extensions lowercase\n",
    "df[\"ext\"] = df.ext.str.lower().astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd693de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_counts = df.groupby(\"ext\")[\"bytes\"].count()\n",
    "all_sizes = df.groupby(\"ext\")[\"bytes\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfec0d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_by_size = all_sizes.nlargest(4)\n",
    "\n",
    "sizes = list(top_by_size.values)\n",
    "extensions = list(top_by_size.index.values)\n",
    "counts = []\n",
    "\n",
    "print(\"Ext\".ljust(10), \"Size\".rjust(11), \"Count\".rjust(10))\n",
    "print(\"---------------------------------\")\n",
    "for i in range(len(sizes)):\n",
    "    size = sizes[i]\n",
    "    ext = extensions[i]\n",
    "    count = all_counts[ext]\n",
    "    counts.append(count)\n",
    "    print((\".\"+ext).ljust(10), \"{:.3f} TB\".format(size/1e12).rjust(11), end=\" \")\n",
    "    print(\"{:10d}\".format(count))\n",
    "print(\"---------------------------------\")\n",
    "print(\n",
    "    \"OTHER\".ljust(10), \n",
    "    \"{:.3f} TB\".format((np.sum(df.bytes) - np.sum(sizes))/1e12).rjust(11),\n",
    "    \"{:10d}\".format(np.sum(all_counts) - np.sum(counts))\n",
    ")\n",
    "print(\n",
    "    \"TOTAL\".ljust(10), \n",
    "    \"{:.3f} TB\".format(np.sum(df.bytes)/1e12).rjust(11),\n",
    "    \"{:10d}\".format(np.sum(all_counts))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f8731a",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_by_count = all_counts.nlargest(6)\n",
    "\n",
    "sizes = []\n",
    "extensions = list(top_by_count.index.values)\n",
    "counts = list(top_by_count.values)\n",
    "\n",
    "print(\"Ext\".ljust(10), \"Count\".rjust(10), \"Size\".rjust(11))\n",
    "print(\"---------------------------------\")\n",
    "for i in range(len(counts)):\n",
    "    count = counts[i]\n",
    "    ext = extensions[i]\n",
    "    size = all_sizes[ext]\n",
    "    sizes.append(size)\n",
    "    print((\".\"+ext).ljust(10), \"{:10d}\".format(count), end=\" \")\n",
    "    print(\"{:.3f} TB\".format(size/1e12).rjust(11))\n",
    "print(\"---------------------------------\")\n",
    "print(\n",
    "    \"OTHER\".ljust(10), \n",
    "    \"{:10d}\".format(np.sum(all_counts) - np.sum(counts)),\n",
    "    \"{:.3f} TB\".format((np.sum(df.bytes) - np.sum(sizes))/1e12).rjust(11)\n",
    ")\n",
    "print(\n",
    "    \"TOTAL\".ljust(10),\n",
    "    \"{:10d}\".format(np.sum(all_counts)),\n",
    "    \"{:.3f} TB\".format(np.sum(df.bytes)/1e12).rjust(11)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393cff41",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(12, 9))\n",
    "\n",
    "counts, bin_edges, _ = axes.hist(df.bytes, bins=BINS);\n",
    "axes.set_xscale(\"log\");\n",
    "axes.set_xticks(BINS);\n",
    "axes.set_xticklabels(LABELS)\n",
    "\n",
    "axes.set_ylabel(\"Count\", size=20);\n",
    "axes.set_xlabel(\"File Size\", size=20);\n",
    "\n",
    "plt.title(\"{0}/.../* [{1:.1f} TB]\".format(HADOOP_PATH, np.sum(df.bytes)/1e12), size=20);\n",
    "\n",
    "label_locs = bin_edges[:-1]*3\n",
    "for i, count in enumerate(counts.astype(int)):\n",
    "    plt.text(label_locs[i], count+2e3, \"{:,d}\".format(count), horizontalalignment='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d85e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(12, 9))\n",
    "plt.tick_params(labelsize=14)\n",
    "\n",
    "file_types = list((df.groupby(\"ext\").bytes.count()).nlargest(6).index.values)\n",
    "counts, bin_edges, _ = axes.hist(\n",
    "    [df[df.ext == e].bytes for e in file_types]+[df[~df.ext.isin(file_types)].bytes], \n",
    "    stacked=True,\n",
    "    bins=BINS,\n",
    "    label=file_types+[\"other\"]\n",
    ");\n",
    "axes.legend(fontsize=14);\n",
    "axes.set_xscale(\"log\");\n",
    "axes.set_xticks(BINS);\n",
    "axes.set_xticklabels(LABELS);\n",
    "\n",
    "axes.set_ylabel(r\"Count\", size=20);\n",
    "axes.set_xlabel(\"File Size\", size=20);\n",
    "\n",
    "plt.title(\"{0} [{1:.1f} TB]\".format(HADOOP_PATH, np.sum(df.bytes)/1e12), size=20);\n",
    "\n",
    "total_counts = counts[-1].astype(int)\n",
    "total = np.sum(total_counts)\n",
    "print(\"Bucket\".ljust(8), \"Count\".rjust(7), end=\" \")\n",
    "print(\"Count%\".rjust(8))\n",
    "print(\"-------------------------\")\n",
    "for i, count in enumerate(total_counts):\n",
    "    print(\n",
    "        \"< {}\".format(LABELS[i+1]).ljust(8), \n",
    "        \"{:7d}\".format(count),\n",
    "        end=\" \"\n",
    "    )\n",
    "    print(\"{:.2f}%\".format(count/total*100).rjust(8))\n",
    "print(\"-------------------------\")\n",
    "print(\"TOTAL\".ljust(8), \"{:7d}\".format(total), end=\" \")\n",
    "print(\"100.00%\".rjust(8))\n",
    "\n",
    "label_locs = bin_edges[:-1]*3\n",
    "for i, count in enumerate(total_counts):\n",
    "    plt.text(label_locs[i], count+2e3, \"{:,d}\".format(count), horizontalalignment='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c23b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(12, 9))\n",
    "plt.tick_params(labelsize=14)\n",
    "\n",
    "USER = \"jguiang\"\n",
    "\n",
    "file_types = list((df[df.user == USER].groupby(\"ext\").bytes.count()).nlargest(6).index.values)\n",
    "counts, bin_edges, _ = axes.hist(\n",
    "    [df[(df.user == USER) & (df.ext == e)].bytes for e in file_types]+[df[(df.user == USER) & ~df.ext.isin(file_types)].bytes], \n",
    "    stacked=True,\n",
    "    bins=BINS,\n",
    "    label=file_types+[\"other\"]\n",
    ");\n",
    "axes.legend(fontsize=14);\n",
    "axes.set_xscale(\"log\");\n",
    "axes.set_xticks(BINS);\n",
    "axes.set_xticklabels(LABELS);\n",
    "\n",
    "axes.set_ylabel(r\"Count\", size=20);\n",
    "axes.set_xlabel(\"File Size\", size=20);\n",
    "\n",
    "before_rep = np.sum(df[df.user == USER].bytes)/1e12\n",
    "plt.title(f\"{HADOOP_PATH}/{USER} [{before_rep:.1f} TB Before Repl.]\", size=20);\n",
    "\n",
    "total_counts = counts[-1].astype(int)\n",
    "total = np.sum(total_counts)\n",
    "print(\"Bucket\".ljust(8), \"Count\".rjust(7), end=\" \")\n",
    "print(\"Count%\".rjust(8))\n",
    "print(\"-------------------------\")\n",
    "for i, count in enumerate(total_counts):\n",
    "    print(\n",
    "        \"< {}\".format(LABELS[i+1]).ljust(8), \n",
    "        \"{:7d}\".format(count),\n",
    "        end=\" \"\n",
    "    )\n",
    "    print(\"{:.2f}%\".format(count/total*100).rjust(8))\n",
    "print(\"-------------------------\")\n",
    "print(\"TOTAL\".ljust(8), \"{:7d}\".format(total), end=\" \")\n",
    "print(\"100.00%\".rjust(8))\n",
    "\n",
    "label_locs = bin_edges[:-1]*3\n",
    "for i, count in enumerate(total_counts):\n",
    "    plt.text(label_locs[i], count, \"{:,d}\".format(count), horizontalalignment='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371a1054",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(12, 9))\n",
    "\n",
    "counts, bin_edges, _ = axes.hist(df[df.ext == \"root\"].bytes, bins=BINS);\n",
    "axes.set_xscale(\"log\");\n",
    "axes.set_xticks(BINS);\n",
    "axes.set_xticklabels(LABELS)\n",
    "\n",
    "axes.set_ylabel(\"Count\", size=20);\n",
    "axes.set_xlabel(\"File Size\", size=20);\n",
    "\n",
    "plt.title(\"{0}/.../*.root [{1:.1f} TB]\".format(HADOOP_PATH, np.sum(df[df.ext == \"root\"].bytes)/1e12), size=20);\n",
    "\n",
    "label_locs = bin_edges[:-1]*3\n",
    "for i, count in enumerate(counts.astype(int)):\n",
    "    plt.text(label_locs[i], count+2e3, \"{:,d}\".format(count), horizontalalignment='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedb88fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(12, 9))\n",
    "\n",
    "counts, bin_edges, _ = axes.hist(df[df.ext != \"root\"].bytes, bins=BINS);\n",
    "axes.set_xscale(\"log\");\n",
    "axes.set_xticks(BINS);\n",
    "axes.set_xticklabels(LABELS)\n",
    "\n",
    "axes.set_ylabel(\"Count\", size=20);\n",
    "axes.set_xlabel(\"File Size\", size=20);\n",
    "\n",
    "plt.title(\"{0}/.../*{{!.root}} [{1:.1f} TB]\".format(HADOOP_PATH, np.sum(df[df.ext != \"root\"].bytes)/1e12), size=20);\n",
    "\n",
    "label_locs = bin_edges[:-1]*3\n",
    "for i, count in enumerate(counts.astype(int)):\n",
    "    plt.text(label_locs[i], count+1e3, \"{:,d}\".format(count), horizontalalignment='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d5f7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df[(df.ext != \"root\") & (df.bytes > BYTES_LOOKUP[\"1GB\"])].groupby(\"ext\")[\"bytes\"].sum()/1e12).nlargest(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c4c8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [\n",
    "    1*1e0, 10*1e0, 100*1e0, # b\n",
    "    1*1e3, 10*1e3, 100*1e3, # kb\n",
    "    1*1e6, 10*1e6, 100*1e6, # mb\n",
    "    1*1e9, 10*1e9, 100*1e9, # gb\n",
    "    1*1e12, 10*1e12, # tb\n",
    "]\n",
    "\n",
    "def num_to_human(num, rjust=5):\n",
    "    if num < 1000**1:   \n",
    "        return f\"{int(num/1000**0)}B\".rjust(rjust)\n",
    "    elif num < 1000**2:   \n",
    "        return f\"{int(num/1000**1)}KB\".rjust(rjust)\n",
    "    elif num < 1000**3:   \n",
    "        return f\"{int(num/1000**2)}MB\".rjust(rjust)\n",
    "    elif num < 1000**4:   \n",
    "        return f\"{int(num/1000**3)}GB\".rjust(rjust)\n",
    "    elif num < 1000**5:   \n",
    "        return f\"{int(num/1000**4)}TB\".rjust(rjust)\n",
    "    else:\n",
    "        return num\n",
    "\n",
    "def interval_to_human(interval):\n",
    "    left = num_to_human(interval.left, rjust=0)\n",
    "    right = num_to_human(interval.right, rjust=0)\n",
    "    return f\"<{right}\"\n",
    "\n",
    "\n",
    "df[\"bucket\"] = pd.cut(df[\"bytes\"], BINS, right=False)\n",
    "df_bucketed = df[df.ext != \"root\"].groupby(\"bucket\")[\"bytes\"].agg([\"count\", \"sum\"]).reset_index()\n",
    "df_bucketed[\"size\"] = df_bucketed[\"sum\"]/1e9 # to GB\n",
    "df_bucketed = df_bucketed.drop(columns=[\"sum\"])\n",
    "\n",
    "df_bucketed[\"count_pct\"] = 100.*df_bucketed[\"count\"]/df_bucketed[\"count\"].sum()\n",
    "df_bucketed[\"count_pct_cumul\"] = np.cumsum(df_bucketed[\"count_pct\"])\n",
    "\n",
    "df_bucketed[\"size_pct\"] = 100.*df_bucketed[\"size\"]/df_bucketed[\"size\"].sum()\n",
    "df_bucketed[\"size_pct_cumul\"] = np.cumsum(df_bucketed[\"size_pct\"])\n",
    "\n",
    "df_bucketed[\"bucket\"] = df_bucketed[\"bucket\"].map(interval_to_human)\n",
    "\n",
    "# add total row by concating to the bottom\n",
    "total = df_bucketed.sum().to_frame().T.assign(\n",
    "    bucket=\"total\",\n",
    "    size_pct_cumul=100.,\n",
    "    count_pct_cumul=100.,\n",
    ")\n",
    "df_bucketed = pd.concat([df_bucketed, total], axis=0).reset_index(drop=True)\n",
    "\n",
    "# more compact names\n",
    "df_bucketed = df_bucketed.rename(columns={\n",
    "    \"count_pct\": \"count%\",\n",
    "    \"count_pct_cumul\": \"c-count%\",\n",
    "    \"size_pct\": \"size%\",\n",
    "    \"size_pct_cumul\": \"c-size%\",\n",
    "})\n",
    "\n",
    "# order the columns\n",
    "df_bucketed = df_bucketed[[\"bucket\", \"count\", \"count%\", \"c-count%\", \"size\", \"size%\", \"c-size%\"]]\n",
    "\n",
    "df_bucketed\n",
    "\n",
    "df_bucketed.style.format({\n",
    "    \"count\": \"{:,.0f}\".format,\n",
    "    \"count%\": \"{:.2f}%\".format,\n",
    "    \"c-count%\": \"{:.2f}%\".format,\n",
    "    \n",
    "    \"size\": \"{:,.2f} GB\".format,\n",
    "    \"size%\": \"{:.2f}%\".format,\n",
    "    \"c-size%\": \"{:.2f}%\".format,\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "librarian",
   "language": "python",
   "name": "librarian"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
